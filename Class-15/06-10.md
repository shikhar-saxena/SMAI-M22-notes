---
title: Class 15
date: October 06, 2022
---

# Ensemble Methods

Have many samples and combine them to get a result or something.


## Bagging: Bootstrap Aggregation

Points belonging to red/blue class in training set

I want to create multiple classifiers (that are different otherwise we are merely copying). How to create variety in classifier? We change the training data.

### Bootstrap

Sampling the training data with replacement.

Basically choose a sample (***sample same size as the original training set***), use it, then put it back. Then pick sample again.
In such a scenario, some samples might be picked multiple times and some not at all.

Combine the decision from these classifiers and we get the final decision. We'll discuss later how to combine the bootstrap data classifer result. This is known as **bootstrap aggregation**.


- Reduces the variance of an estimated prediction function
- For classification, a committee of classifiers each cast a vote for the predicted class.
- Another advantage of bagging is that the computation can be parallelized.

## Boosting

Another variant of ensemble members. Here each classifer is tuned to be *complementary* to previous ones.

So we see where does a classifier perform well and where does it not perform well. Then we try to generate a classifier that is complementary to this performance.

We combine them using a weighted average (weights proportional to their performance on validation set).

**AdaBoost** is a popular boosting method.

### Example

Choose a weak classifer with boundary equation $h_1(x) = 0$ and $H(x): sign(h_1 (x))$ is the classifier.

We combine them as

$$H(x): sign(\alpha_1 h_1(x) + \alpha_2 h_2(x) + \alpha_3 h_3(x))$$
